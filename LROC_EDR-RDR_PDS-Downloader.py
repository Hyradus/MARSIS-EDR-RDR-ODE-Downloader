#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCRIPT for search and download LROC EDR/RDR files from PDS
@author: Giacomo Nodjoumi - g.nodjoumi@jacobs-university.de


_________________________________ README _________________________________

The script can work both passing some/all arguments or none

Arguments are:
    
--orbits: csv file containing all orbit to download.

This csv can be manually created or generated by QGIS

------ MANUAL CSV
Is as a one-column csv with "Orbit" (without quotes) as first row
and orbit numbers as new rows. e.g.

                                Orbit
                                02445
                                01285
                                12332
                                ...

------ QGIS CSV                                
Simply is a csv created by saving selected features directly in QGIS.
See full readme for better explaination



--ddir: is the folder where the files will be downloaded
default is a folder called "downloads+datetime".
Automatically created in the directory where is executed the script

--dtype: Select EDR or RDR

If NO argument is passed, defaults are used and interactively requested the others.


@author: Giacomo Nodjoumi - g.nodjoumi@jacobs-university.de
"""

import os
import pathlib
import pandas as pd
import geopandas as gpd
from argparse import ArgumentParser
import shutil
from datetime import datetime
from tqdm import tqdm
import urllib.request

from bs4 import BeautifulSoup as BS

def answer(question):
    answ = None
    while answ not in ['yes','y','no','n']:
        print("Please enter yes/y or no/n.")    
        answ = input(question+': ')
    return(answ)

def make_folder(name):
    os.getcwd()
    folder = name
    if os.path.exists(folder):
           qst = name + ' Folder exist, remove it? '
           answ = answer(qst)
           if answ in ['yes', 'y']:               
               shutil.rmtree(folder)
               os.mkdir(folder)
               print(name, 'Folder created')
           else:
               now = datetime.now()
               new_name = name +'_' + now.strftime("%d-%m-%Y_%H-%M-%S")
               print(new_name, ' Folder not exist, creating.')
               os.mkdir(new_name)
               print('Created new ', name,' Folder')
    else:
        print(name, ' Folder not exist, creating.')
        os.mkdir(folder)
        print('Created new ', name,' Folder')
    return(folder)


def readGPKG(gpkg_path):
    
    gpkgDF=gpd.read_file(gpkg_path)
    return(gpkgDF)

# def orbits2urls(gpkgDF):
#     download_urls = []
#     download_size = 0
#     with tqdm(total=len(gpkgDF),
#               desc = 'Creating file urls',
#               unit='File') as pbar:
        
#             urls = gpkgDF['FilesURL'].tolist()
#             for url in urls:
                
#                 # print(df_url)
#                 # FileUrl = getFileUrl(url)
#                 download_urls.append(FileUrl)
#                 pbar.update(1)            
#                 fsize=int(getFileSize(FileUrl))
#                 download_size=download_size+fsize
#             pass
#     return(download_urls, download_size)
    

def getFileUrl(df_url):
    
    ext = '.IMG'
    
    page = urllib.request.urlopen(df_url)
    soup = BS(page, 'html.parser')
    # for link in soup.findAll('a'):
    #     print(link.get('href'))
    FUrl = [link.get('href') for link in soup.findAll('a') if link.get('href').endswith(ext)][0]
    
    return(FUrl)

def getFileSize(FileUrl):
    import ssl
    ssl._create_default_https_context = ssl._create_unverified_context
    
    # FileUrl = getFileUrl(url)
    
    meta= urllib.request.urlopen(FileUrl).info()
    fsize = meta.get('Content-Length')
    print(fsize)
    return(fsize)




def chunk_creator(item_list, chunksize):
    import itertools
    it = iter(item_list)
    while True:
        chunk = tuple(itertools.islice(it, chunksize))
        if not chunk:
            break
        yield chunk


def parallel_funcs(files, JOBS, func):
    from joblib import Parallel, delayed
    result = Parallel (n_jobs=JOBS)(delayed(func)(files[i])
                            for i in range(len(files)))
    return(result)


    
    



    # # file_sizes = []
    # with tqdm(total=len(file_urls),
    #          desc = 'Generating Images',
    #          unit='File') as pbar:
        
    #     chunks = []
    #     for c in chunk_creator(file_urls, JOBS):
    #         chunks.append(c)
    
    #     for i in range(len(chunks)):
    #         files = chunks[i]
    #         results = parallel_funcs(files, JOBS, getFile)
    #         pbar.update(JOBS)
            
    #         # time.sleep(2)
    #         # [file_sizes.append(size) for size in results]



def getFile(url):
    import ssl
    ssl._create_default_https_context = ssl._create_unverified_context
    fname = pathlib.Path(url).name
    savename = fname
    v = 0
    while os.path.exists(savename):
        
        v = v+1
        savename = str(v) +'_'+fname
        urllib.request.urlretrieve(url, fname)
    else:
        urllib.request.urlretrieve(url, savename)
   
        

############################# DOWNLOAD #############################
            

def download(download_urls, download_size, ddir,units):
    # os.chdir(ddir)
    with tqdm(total=download_size, unit=units, unit_scale=True) as pbar:
            for urls in download_urls:
                getFile(urls)
                fsize=int(getFileSize(urls))
                pbar.update(fsize)
















def retreival(gpkgDF):
    # import ssl
    # ssl._create_default_https_context = ssl._create_unverified_context
    # download_urls, download_size = orbits2urls(gpkgDF)
    # units = 'MB'
    # size = float(download_size/1000000)
    # if size > 1000:
    #     size = size/1000
    #     units = 'GB'    
    # print('\n', len(download_urls),' files requested with a size of: ', size, units)
        
    # answ = answer('Proceed with the download?')
    # if answ in ['yes','y']:
    #     download(download_urls, download_size, ddir, units)
    
    # else:
    #     print('\nSkipping download')
    from tqdm import tqdm
    import psutil
    
    os.chdir(ddir)

    JOBS=psutil.cpu_count(logical=False)
    
    
    download_urls = gpkgDF['FilesURL'].tolist()
    # file_urls = [getFileUrl(url) ]
    file_urls = []
    with tqdm(total=len(download_urls),
             desc = 'Generating Images',
             unit='File') as pbar:
        
        chunks = []
        for c in chunk_creator(download_urls, JOBS):
            chunks.append(c)
    
        for i in range(len(chunks)):
            files = chunks[i]
            results = parallel_funcs(files, JOBS, getFileUrl)
            pbar.update(JOBS)
            
            
            [file_urls.append(url) for url in results]


    # file_sizes = []
    # with tqdm(total=len(file_urls),
    #          desc = 'Generating Images',
    #          unit='File') as pbar:
        
    #     chunks = []
    #     for c in chunk_creator(file_urls, JOBS):
    #         chunks.append(c)
    
    #     for i in range(len(chunks)):
    #         files = chunks[i]
    #         results = parallel_funcs(files, JOBS, getFileSize)
    #         pbar.update(JOBS)
            
    #         # time.sleep(2)
    #         [file_sizes.append(size) for size in results]

    # file_sizes = []
    with tqdm(total=len(file_urls),
              desc = 'Downloading Images',
              unit='File') as pbar:
        
        chunks = []
        for c in chunk_creator(file_urls, JOBS):
            chunks.append(c)
    
        for i in range(len(chunks)):
            files = chunks[i]
            results = parallel_funcs(files, JOBS, getFile)
            pbar.update(JOBS)
            
            # time.sleep(2)
            # [file_sizes.append(size) for size in results]

def main(gpkgDF):
   
  
    retreival(gpkgDF)
    
    print('\nAll operations completed')


if __name__ == "__main__":
    global ddir
    
    parser = ArgumentParser()
    parser.add_argument('--pwd', help='password for ftp login')
    parser.add_argument('--orbits', help='Csv with orbits')
    parser.add_argument('--ddir', help='Download folder')
    parser.add_argument('--dtype', help='Data type, EDR or RDR')
    
    args = parser.parse_args()
    pwd=args.pwd
    orbits_file = args.orbits
    ddir = args.ddir
    dtype=args.dtype

    ddir = '/media/hyradus/DATA/TEMP/LROC'
    
    gpkgDF = readGPKG('/media/hyradus/DATA/Syncthing/SyncData/NewDataset/MOON/LROC_PITS_filtered.shp')
    
    main(gpkgDF)
   



